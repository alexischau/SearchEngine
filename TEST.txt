20 Test Queries:
1. “ics” - We wanted a query that would take a while to search, since lots of documents contain this phrase. We also expected that many documents would contain this query, so we used it to test our ranking function. Before implementing our ranking function, the results were not as relevant as they simply contained the first few words that had “ics.”
2. “hamsterdam” - We wanted to test with a query that did not exist in our index, to see if it would also take a while. Our first query was actually “amsterdam,” as we believed that this word would not exist anywhere inside our corpus. To our surprise, it did, which is why we changed it to a nonsensical word, “hamsterdam.” 
3. “a” - We wanted to test a stopword that takes a long time, since we hypothesized that a common stopword would have one of the largest index postings and thus take a large amount of time. This would represent an approximate upper bound of how long our search would take.
4. “a and the” - We also wanted to test 3 stopwords used in conjunction, to see how that would affect the search time and if it would still be possible to retrieve and rank under the 300ms time limit. Previously, we took out all stopwords, which returned no results, so we changed that in our code to decide between removing stopwords to speed up a query (if it has words other than stopwords) or to keep stopwords in a stopwords-only query.
5. “master of software engineering professor is cool” - We tested with a long query to see whether it would take more time. Before adding stopword removal and changing how we factored our code, it exceeded the time limit. After implementing that, it met the time limit.
6. “a and the computer science” -  This is another query with many stopwords, which is expected to take time as there are many documents with stopwords in our index. It took around 22 ms before implementing stopword removal, and around 15 ms after.
7. “shannon alfaro” -  We wanted to search for a very specific result in order to improve the ranking. After changing the title and weighting it differently than normal text, we were able to find and rank specific articles that are about Professor Alfaro.
8. “machine learning” - A word in ICS with a specific meaning, but separated they mean different things. Was hoping to retrieve pages that dealt with ML rather than pages about students, machines, or students learning about machines. The results that we found were relevant and related to the results we wanted.
9. “to be or not to be” - We wanted to test another phrase that contained only stopwords to see if we were able to find results, and if it would be under our time limit.
10. “meme doge slay” - We wanted to test a conjunction of 3 words that will not be in our index. 
11. “water machine” - We wanted to test 2 words we expect to find in our queries, and see the results, in order to test query 12 to see if changes in the term frequency would change the results.
12. “water water machine” - repeating schedule in the query to see if there’s any effect on the words. The addition of a second “water” term in the query did change the results, with results containing “water” being ranked higher.
13. “women in information and computer science” - We used this specific query in mind of finding WICS-related pages. We successfully found blog posts that talked about the club, as well as other posts about women and computers such as computing awards, technology scholarship, and gaming.
14. “research” - We used this query to determine the relevance of our results, and whether our weighting of header and bolded terms would change the ranking. Our first result turned out to be the UCI CS faculty page, where it had research areas of each faculty in bold.
15. “graduate research” - Compared to query 14, we added the word “graduate” to test conjunction and whether the results were relevant to a more specific query of graduates and research.
16. “professor research award” - This also adds more specificity to the query. We had a certain result in mind (finding blogs or posts about professors who do research or won awards), and our results matched our expectations.
17. “debra richardson” - We wanted to find a specific site dedicated to her in order to evaluate ranking. This page should be on the top of the query, and in our results, it was her faculty page.
18. “ics computer science information class university irvine professor student” - We wanted to test some terms that would be common in our index with a long query. This query met the time limit.
19. “she will go in to the computer store” - This is a query that contains mostly stopwords, since we expected it to take longer than usual due to how common stopwords are. We wanted to test the time it took for this, which was under 300ms.
20. “irvine” - This a query that we tested relevancy on, specifically our weighting function. Since “irvine” is a common term within our corpus of UCI webpages, we wanted to make sure that the results we have featured “irvine” in important positions. The top results that our search returned contained “irvine” in the title.